# -*- coding: utf-8 -*-
"""app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SFh9aZAWVlSFGb--xPKmZpihSQvedsKr
"""

import pdfplumber
import re
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline
import streamlit as st
from langchain.text_splitter import RecursiveCharacterTextSplitter

pdf_file = "Small Business Relief Guide - EN - 29 08 2023.pdf"  # Replace with your PDF path
raw_text = ""

with pdfplumber.open(pdf_file) as pdf:
    for page in pdf.pages:
        page_text = page.extract_text()
        if page_text:
            raw_text += page_text + "\n"

def clean_pdf_text(text):
    # Remove footnotes, page numbers, brackets [123] or (123)
    text = re.sub(r'\[\d+\]|\(\d+\)', '', text)
    # Remove standalone numbers like 0s or 1s
    text = re.sub(r'\b\d+s?\b', '', text)
    # Remove boilerplate phrases
    ignore_patterns = [
        r'Contents', r'Introduction', r'Status of this guide',
        r'not a legally binding document'
    ]
    for pattern in ignore_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    # Remove extra spaces
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

clean_text = clean_pdf_text(raw_text)

def deduplicate_sentences(text):
    sentences = re.split(r'(?<=[.!?])\s+', text)
    seen = set()
    unique_sentences = []
    for s in sentences:
        s_clean = s.strip()
        if s_clean and s_clean not in seen:
            seen.add(s_clean)
            unique_sentences.append(s_clean)
    return " ".join(unique_sentences)

clean_text = deduplicate_sentences(clean_text)

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,   # chunk size small enough for T5-small
    chunk_overlap=100 # overlap ensures continuity
)
chunks = text_splitter.split_text(clean_text)
chunks = [deduplicate_sentences(chunk) for chunk in chunks]

embed_model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = embed_model.encode(chunks, show_progress_bar=True)

dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(np.array(embeddings, dtype='float32'))
print(f"FAISS index contains {index.ntotal} vectors")

model_name = "google/flan-t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
qa_pipeline = pipeline("text2text-generation", model=model, tokenizer=tokenizer)

def answer_question_clean(question, top_k=5, max_input_chars=1500, max_new_tokens=250, bullet_points=True):
    # 1. Embed question
    query_emb = embed_model.encode([question])[0]

    # 2. Retrieve top_k relevant chunks
    D, I = index.search(np.array([query_emb], dtype='float32'), top_k)
    top_chunks = [chunks[i] for i in I[0]]

    # 3. Combine, deduplicate, truncate
    context_text = deduplicate_sentences(" ".join(top_chunks))
    context_text = context_text[:max_input_chars]

    # 4. Build prompt
    prompt = f"""
Answer the question based ONLY on the following text.
Do not include unrelated information. List all points clearly and completely.

Text: {context_text}

Question: {question}
"""
    # 5. Generate answer
    answer = qa_pipeline(prompt, max_new_tokens=max_new_tokens)[0]['generated_text']

    # 6. Clean final answer
    answer = clean_pdf_text(answer)

    # 7. Optional: format as bullet points
    if bullet_points:
        sentences = [s.strip() for s in re.split(r'(?<=[.!?])\s+', answer) if s]
        if sentences:
            answer = "\n" + "\n ".join(sentences)

    return answer

print(answer_question_clean("what is criteria to obtain small bussiness relief"))

# --- Example usage ---
print(answer_question_clean("Explain Financial links between entities"))
print(answer_question("What is Business Restructuring Relief?"))

print(answer_question_clean("who is not eligible for small bussiness relief"))

# --------------------------
st.title("Corporate Tax Chatbot")

st.markdown(
    """
Ask any question about corporate tax based on your PDF guide.
The chatbot will provide concise and complete answers.
"""
)

question = st.text_input("Your question:")
if question:
    answer = answer_question_clean(question)
    st.markdown(f"**Answer:**\n{answer}")

# Optional: keep Q&A history
if "history" not in st.session_state:
    st.session_state.history = []

if question:
    st.session_state.history.append((question, answer))

for q, a in st.session_state.history:
    st.markdown(f"**Q:** {q}")
    st.markdown(f"**A:** {a}")
